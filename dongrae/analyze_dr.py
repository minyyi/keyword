"""
ì›¹ì‚¬ì´íŠ¸ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê²€ìƒ‰ê°€ëŠ¥ì„± ë¶„ì„ ë„êµ¬
- íŠ¹ì • ì›¹ì‚¬ì´íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
- ê²€ìƒ‰ì—”ì§„ì—ì„œ í•´ë‹¹ ì‚¬ì´íŠ¸ê°€ ê²€ìƒ‰ë˜ëŠ”ì§€ í™•ì¸
- SEO/GEO ìµœì í™”ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ ë¶„ì„
"""

import requests
from bs4 import BeautifulSoup
import re
import csv
from collections import Counter
from urllib.parse import urljoin, urlparse
import time
import json
from typing import List, Dict, Set

class WebsiteKeywordExtractor:
    """ì›¹ì‚¬ì´íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ê²€ìƒ‰ê°€ëŠ¥ì„± ë¶„ì„"""
    
    def __init__(self, website_url: str):
        self.website_url = website_url
        self.domain = urlparse(website_url).netloc
        self.extracted_keywords = {}
        self.search_results = {}
        
    def fetch_website_content(self) -> Dict:
        """ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
        print(f"ğŸ” ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘: {self.website_url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(self.website_url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
                title = soup.find('title').get_text() if soup.find('title') else ""
                meta_description = ""
                meta_keywords = ""
                
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                if meta_desc:
                    meta_description = meta_desc.get('content', '')
                
                meta_kw = soup.find('meta', attrs={'name': 'keywords'})
                if meta_kw:
                    meta_keywords = meta_kw.get('content', '')
                
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                for script in soup(["script", "style"]):
                    script.decompose()
                
                body_text = soup.get_text()
                
                # í—¤ë”© íƒœê·¸ ì¶”ì¶œ
                headings = {}
                for i in range(1, 7):
                    h_tags = soup.find_all(f'h{i}')
                    headings[f'h{i}'] = [tag.get_text().strip() for tag in h_tags]
                
                # ë§í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                links = soup.find_all('a')
                link_texts = [link.get_text().strip() for link in links if link.get_text().strip()]
                
                result = {
                    'status': 'success',
                    'title': title.strip(),
                    'meta_description': meta_description.strip(),
                    'meta_keywords': meta_keywords.strip(),
                    'body_text': body_text.strip(),
                    'headings': headings,
                    'link_texts': link_texts,
                    'url': self.website_url
                }
                
                print(f"   âœ… ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ì™„ë£Œ")
                print(f"   ğŸ“„ ì œëª©: {title[:50]}...")
                print(f"   ğŸ“ ë©”íƒ€ ì„¤ëª…: {meta_description[:50]}...")
                
                return result
                
            else:
                print(f"   âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
                return {'status': 'error', 'message': f'HTTP {response.status_code}'}
                
        except Exception as e:
            print(f"   âŒ ì›¹ì‚¬ì´íŠ¸ ì ‘ê·¼ ì˜¤ë¥˜: {e}")
            return {'status': 'error', 'message': str(e)}
    
    def extract_keywords_from_content(self, content: Dict) -> List[str]:
        """ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        print("ğŸ”¤ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        
        if content['status'] != 'success':
            return []
        
        all_text = ""
        keywords = []
        
        # ì œëª©ì—ì„œ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
        title_keywords = self.extract_keywords_from_text(content['title'])
        keywords.extend(title_keywords * 3)  # ì œëª© í‚¤ì›Œë“œì— 3ë°° ê°€ì¤‘ì¹˜
        
        # ë©”íƒ€ í‚¤ì›Œë“œ
        if content['meta_keywords']:
            meta_kw = [kw.strip() for kw in content['meta_keywords'].split(',')]
            keywords.extend(meta_kw)
        
        # ë©”íƒ€ ì„¤ëª…ì—ì„œ í‚¤ì›Œë“œ
        desc_keywords = self.extract_keywords_from_text(content['meta_description'])
        keywords.extend(desc_keywords * 2)  # ë©”íƒ€ ì„¤ëª…ì— 2ë°° ê°€ì¤‘ì¹˜
        
        # í—¤ë”©ì—ì„œ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
        for heading_level, headings in content['headings'].items():
            for heading in headings:
                heading_keywords = self.extract_keywords_from_text(heading)
                weight = 3 if heading_level in ['h1', 'h2'] else 2
                keywords.extend(heading_keywords * weight)
        
        # ë³¸ë¬¸ì—ì„œ í‚¤ì›Œë“œ
        body_keywords = self.extract_keywords_from_text(content['body_text'])
        keywords.extend(body_keywords)
        
        # ë§í¬ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ
        for link_text in content['link_texts']:
            link_keywords = self.extract_keywords_from_text(link_text)
            keywords.extend(link_keywords)
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        keyword_counter = Counter(keywords)
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        top_keywords = [kw for kw, count in keyword_counter.most_common(100)]
        
        print(f"   âœ… {len(top_keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        print("   ğŸ” ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ:")
        for i, (keyword, count) in enumerate(keyword_counter.most_common(10), 1):
            print(f"      {i}. {keyword} ({count}íšŒ)")
        
        return top_keywords
    
    def extract_keywords_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text:
            return []
        
        # í…ìŠ¤íŠ¸ ì •ì œ
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ë‹¨ì–´ ë¶„ë¦¬
        words = text.split()
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ìˆ', 'ìˆëŠ”', 'ì—†ëŠ”', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼',
            'ë„', 'ë§Œ', 'ì€', 'ëŠ”', 'ì´ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤', 'ë‹¤ë¥¸', 'ì´ëŸ°', 'ê·¸ëŸ°',
            'ì €ëŸ°', 'ì–´ë–¤', 'ë¬´ì—‡', 'ëˆ„êµ¬', 'ì–¸ì œ', 'ì–´ë””', 'ì–´ë–»ê²Œ', 'ì™œ', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ',
            'ë˜ëŠ”', 'ë˜', 'ë°', 'ë“±', 'ê²ƒë“¤', 'ëª¨ë“ ', 'ê°', 'ì—¬ëŸ¬', 'ë‹¤ì–‘í•œ', 'ë§ì€', 'ì ì€', 'í¬ë‹¤', 'ì‘ë‹¤'
        }
        
        # ìœ íš¨í•œ í‚¤ì›Œë“œ í•„í„°ë§
        keywords = []
        for word in words:
            # ê¸¸ì´ ì¡°ê±´ (2ê¸€ì ì´ìƒ 10ê¸€ì ì´í•˜)
            if 2 <= len(word) <= 10:
                # ë¶ˆìš©ì–´ê°€ ì•„ë‹˜
                if word.lower() not in stopwords:
                    # ìˆ«ìë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŒ
                    if not word.isdigit():
                        keywords.append(word)
        
        return keywords
    
    def check_search_visibility(self) -> Dict:
        """ê²€ìƒ‰ì—”ì§„ì—ì„œ ì›¹ì‚¬ì´íŠ¸ ê²€ìƒ‰ ê°€ëŠ¥ì„± í™•ì¸"""
        print("ğŸ” ê²€ìƒ‰ ê°€ëŠ¥ì„± ë¶„ì„ ì¤‘...")
        
        search_queries = [
            self.domain,
            f'site:{self.domain}',
            'ë²•ë¬´ë²•ì¸ ë™ë˜',
            'ë™ë˜ ë²•ë¬´ë²•ì¸',
            'ì—°ì œ ë²•ë¬´ë²•ì¸',
            'dongraelaw'
        ]
        
        visibility_results = {}
        
        for query in search_queries:
            print(f"   ğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {query}")
            
            # ì‹¤ì œ ê²€ìƒ‰ API í˜¸ì¶œì€ ë³µì¡í•˜ë¯€ë¡œ, ì‹œë®¬ë ˆì´ì…˜
            # ì‹¤ì œë¡œëŠ” Google Custom Search APIë‚˜ ë‹¤ë¥¸ ê²€ìƒ‰ API ì‚¬ìš©
            visibility_results[query] = {
                'query': query,
                'found': 'ì‹œë®¬ë ˆì´ì…˜', # ì‹¤ì œë¡œëŠ” True/False
                'position': 'í…ŒìŠ¤íŠ¸ í•„ìš”',
                'snippet': 'ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ í•„ìš”'
            }
        
        print("   âš ï¸  ì‹¤ì œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ì€ ìˆ˜ë™ìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤:")
        print("   1. Googleì—ì„œ ì§ì ‘ ê²€ìƒ‰")
        print("   2. Naverì—ì„œ ì§ì ‘ ê²€ìƒ‰")
        print("   3. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì‚¬ì´íŠ¸ í™•ì¸")
        
        return visibility_results
    
    def generate_seo_keywords(self, existing_keywords: List[str]) -> List[str]:
        """SEO ìµœì í™”ë¥¼ ìœ„í•œ ì¶”ê°€ í‚¤ì›Œë“œ ìƒì„±"""
        print("ğŸš€ SEO ìµœì í™” í‚¤ì›Œë“œ ìƒì„± ì¤‘...")
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ íŒ¨í„´
        base_patterns = [
            "ë²•ë¬´ë²•ì¸", "ë³€í˜¸ì‚¬", "ë²•ë¥ ìƒë‹´", "ì†Œì†¡", "ë³€í˜¸", "ë²•ë¬´"
        ]
        
        # ì§€ì—­ í‚¤ì›Œë“œ
        location_patterns = [
            "ë¶€ì‚°", "ì—°ì œ", "ì—°ì œêµ¬", "ê±°ì œì—­", "ì‹œì²­ì—­", "ë™ë˜"
        ]
        
        # ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ  
        service_patterns = [
            "ìƒë‹´", "ë¹„ìš©", "ìˆ˜ì„ë£Œ", "ì¶”ì²œ", "í›„ê¸°", "í‰ê°€", "ìˆœìœ„",
            "ë¯¼ì‚¬", "í˜•ì‚¬", "ì´í˜¼", "ìƒì†", "êµí†µì‚¬ê³ ", "ë¶€ë™ì‚°", "ê¸°ì—…ë²•ë¬´"
        ]
        
        # ì˜ë„ í‚¤ì›Œë“œ
        intent_patterns = [
            "ë°©ë²•", "ì ˆì°¨", "ê³¼ì •", "ì¤€ë¹„", "ì‹ ì²­", "ì˜ë¢°", "ì„ ì„", "ë¬¸ì˜"
        ]
        
        generated_keywords = []
        
        # íŒ¨í„´ ì¡°í•©ìœ¼ë¡œ í‚¤ì›Œë“œ ìƒì„±
        for base in base_patterns:
            for location in location_patterns:
                generated_keywords.extend([
                    f"{location} {base}",
                    f"{base} {location}",
                ])
        
        for service in service_patterns:
            for location in location_patterns:
                generated_keywords.extend([
                    f"{location} {service}",
                    f"{location} ë³€í˜¸ì‚¬ {service}",
                    f"{location} ë²•ë¬´ë²•ì¸ {service}"
                ])
        
        # ê¸°ì¡´ í‚¤ì›Œë“œì™€ ì¡°í•©
        for existing in existing_keywords[:10]:  # ìƒìœ„ 10ê°œë§Œ ì‚¬ìš©
            for intent in intent_patterns:
                generated_keywords.extend([
                    f"{existing} {intent}",
                    f"{intent} {existing}"
                ])
        
        # ì¤‘ë³µ ì œê±°
        unique_generated = list(set(generated_keywords))
        
        print(f"   âœ… {len(unique_generated)}ê°œ SEO í‚¤ì›Œë“œ ìƒì„±")
        
        return unique_generated
    
    def save_analysis_results(self, content: Dict, keywords: List[str], 
                            seo_keywords: List[str], visibility: Dict, 
                            filename: str = "website_analysis.csv"):
        """ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ë¥¼ {filename}ì— ì €ì¥ ì¤‘...")
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.writer(csvfile)
                
                # ì›¹ì‚¬ì´íŠ¸ ê¸°ë³¸ ì •ë³´
                writer.writerow(['=== ì›¹ì‚¬ì´íŠ¸ ê¸°ë³¸ ì •ë³´ ==='])
                writer.writerow(['í•­ëª©', 'ë‚´ìš©'])
                writer.writerow(['URL', self.website_url])
                writer.writerow(['ì œëª©', content.get('title', '')])
                writer.writerow(['ë©”íƒ€ ì„¤ëª…', content.get('meta_description', '')])
                writer.writerow(['ë©”íƒ€ í‚¤ì›Œë“œ', content.get('meta_keywords', '')])
                writer.writerow([])
                
                # ì¶”ì¶œëœ í‚¤ì›Œë“œ
                writer.writerow(['=== ì¶”ì¶œëœ í‚¤ì›Œë“œ ==='])
                writer.writerow(['ìˆœë²ˆ', 'í‚¤ì›Œë“œ', 'ìœ í˜•'])
                
                for i, keyword in enumerate(keywords, 1):
                    writer.writerow([i, keyword, 'ì›¹ì‚¬ì´íŠ¸ ì¶”ì¶œ'])
                
                writer.writerow([])
                
                # SEO í‚¤ì›Œë“œ
                writer.writerow(['=== SEO ìµœì í™” í‚¤ì›Œë“œ ==='])
                writer.writerow(['ìˆœë²ˆ', 'í‚¤ì›Œë“œ', 'ìœ í˜•'])
                
                for i, keyword in enumerate(seo_keywords, 1):
                    writer.writerow([i, keyword, 'SEO ìƒì„±'])
                
                writer.writerow([])
                
                # ê²€ìƒ‰ ê°€ëŠ¥ì„±
                writer.writerow(['=== ê²€ìƒ‰ ê°€ëŠ¥ì„± ë¶„ì„ ==='])
                writer.writerow(['ê²€ìƒ‰ì–´', 'ê²°ê³¼', 'ë¹„ê³ '])
                
                for query, result in visibility.items():
                    writer.writerow([query, result['found'], 'ìˆ˜ë™ í™•ì¸ í•„ìš”'])
            
            print(f"   âœ… ë¶„ì„ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë¨")
            
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("=" * 60)
        print(f"ğŸš€ ì›¹ì‚¬ì´íŠ¸ í‚¤ì›Œë“œ ë¶„ì„ ì‹œì‘: {self.website_url}")
        print("=" * 60)
        
        # 1. ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
        print("\n1ï¸âƒ£ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ë¶„ì„")
        content = self.fetch_website_content()
        
        if content['status'] != 'success':
            print(f"âŒ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤íŒ¨: {content.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            return
        
        # 2. í‚¤ì›Œë“œ ì¶”ì¶œ
        print("\n2ï¸âƒ£ ì›¹ì‚¬ì´íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ")
        extracted_keywords = self.extract_keywords_from_content(content)
        
        # 3. SEO í‚¤ì›Œë“œ ìƒì„±
        print("\n3ï¸âƒ£ SEO ìµœì í™” í‚¤ì›Œë“œ ìƒì„±")
        seo_keywords = self.generate_seo_keywords(extracted_keywords)
        
        # 4. ê²€ìƒ‰ ê°€ëŠ¥ì„± í™•ì¸
        print("\n4ï¸âƒ£ ê²€ìƒ‰ ê°€ëŠ¥ì„± ë¶„ì„")
        visibility_results = self.check_search_visibility()
        
        # 5. ê²°ê³¼ ì €ì¥
        print("\n5ï¸âƒ£ ê²°ê³¼ ì €ì¥")
        filename = f"{self.domain.replace('.', '_')}_analysis.csv"
        self.save_analysis_results(content, extracted_keywords, seo_keywords, 
                                 visibility_results, filename)
        
        # 6. ê²°ê³¼ ìš”ì•½
        print("\n6ï¸âƒ£ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“Š ì¶”ì¶œëœ í‚¤ì›Œë“œ: {len(extracted_keywords)}ê°œ")
        print(f"ğŸš€ SEO í‚¤ì›Œë“œ: {len(seo_keywords)}ê°œ")
        print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {filename}")
        
        print(f"\nğŸ“‹ ì¶”ì²œ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. CSV íŒŒì¼ì„ ì—‘ì…€ì—ì„œ ì—´ì–´ì„œ í‚¤ì›Œë“œ ê²€í† ")
        print("2. Google/Naverì—ì„œ ì§ì ‘ ê²€ìƒ‰í•´ì„œ ì‚¬ì´íŠ¸ ë…¸ì¶œ í™•ì¸")
        print("3. íš¨ê³¼ì ì¸ í‚¤ì›Œë“œë¡œ ì½˜í…ì¸  ìµœì í™”")
        print("4. AI ê²€ìƒ‰(ChatGPT/Claude)ì—ì„œ ë¸Œëœë“œ ì–¸ê¸‰ë„ í…ŒìŠ¤íŠ¸")
        
        return {
            'content': content,
            'extracted_keywords': extracted_keywords,
            'seo_keywords': seo_keywords,
            'visibility': visibility_results,
            'filename': filename
        }

# ì‹¤í–‰ í•¨ìˆ˜
def analyze_dongraelaw_website():
    """ë²•ë¬´ë²•ì¸ ë™ë˜ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤í–‰"""
    
    website_url = "https://www.dongraelaw.shop/"
    
    analyzer = WebsiteKeywordExtractor(website_url)
    results = analyzer.run_full_analysis()
    
    return results

# ìˆ˜ë™ ê²€ìƒ‰ ê°€ì´ë“œ í•¨ìˆ˜
def manual_search_guide():
    """ìˆ˜ë™ ê²€ìƒ‰ í™•ì¸ ê°€ì´ë“œ"""
    print("\n" + "="*50)
    print("ğŸ” ìˆ˜ë™ ê²€ìƒ‰ í™•ì¸ ê°€ì´ë“œ")
    print("="*50)
    
    search_tests = [
        ("Google", "dongraelaw.shop", "https://www.google.com/search?q=dongraelaw.shop"),
        ("Google", "ë²•ë¬´ë²•ì¸ ë™ë˜", "https://www.google.com/search?q=ë²•ë¬´ë²•ì¸+ë™ë˜"),
        ("Naver", "ë²•ë¬´ë²•ì¸ ë™ë˜", "https://search.naver.com/search.naver?query=ë²•ë¬´ë²•ì¸+ë™ë˜"),
        ("Google", "ì—°ì œ ë²•ë¬´ë²•ì¸", "https://www.google.com/search?q=ì—°ì œ+ë²•ë¬´ë²•ì¸"),
        ("Google", "site:dongraelaw.shop", "https://www.google.com/search?q=site:dongraelaw.shop")
    ]
    
    print("ë‹¤ìŒ ê²€ìƒ‰ë“¤ì„ ì§ì ‘ í•´ë³´ì„¸ìš”:")
    print()
    
    for i, (engine, query, url) in enumerate(search_tests, 1):
        print(f"{i}. {engine}ì—ì„œ '{query}' ê²€ìƒ‰")
        print(f"   URL: {url}")
        print(f"   í™•ì¸ì‚¬í•­: dongraelaw.shop ì‚¬ì´íŠ¸ê°€ ê²°ê³¼ì— ë‚˜ì˜¤ëŠ”ì§€")
        print()
    
    print("âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸:")
    print("â–¡ ë¸Œëœë“œëª…ìœ¼ë¡œ ê²€ìƒ‰ ì‹œ 1í˜ì´ì§€ì— ë‚˜ì˜¤ëŠ”ê°€?")
    print("â–¡ ì§€ì—­ëª…ìœ¼ë¡œ ê²€ìƒ‰ ì‹œ ê²½ìŸì‚¬ë“¤ê³¼ í•¨ê»˜ ë‚˜ì˜¤ëŠ”ê°€?") 
    print("â–¡ site: ê²€ìƒ‰ìœ¼ë¡œ í˜ì´ì§€ë“¤ì´ ìƒ‰ì¸ë˜ì–´ ìˆëŠ”ê°€?")
    print("â–¡ íšŒì‚¬ëª… ì •í™•íˆ ì…ë ¥ ì‹œ ìµœìƒë‹¨ì— ë‚˜ì˜¤ëŠ”ê°€?")

if __name__ == "__main__":
    print("ğŸš€ ë²•ë¬´ë²•ì¸ ë™ë˜ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    
    # ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ì‹¤í–‰
    results = analyze_dongraelaw_website()
    
    # ìˆ˜ë™ ê²€ìƒ‰ ê°€ì´ë“œ ì¶œë ¥
    manual_search_guide()