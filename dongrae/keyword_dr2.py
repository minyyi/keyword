"""
ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë„êµ¬
- LLM ì—†ì´ í‚¤ì›Œë“œ ì¶”ì¶œ
- í„°ë¯¸ë„ì—ì„œ ê²°ê³¼ í™•ì¸
- ë²•ë¬´ë²•ì¸ ë™ë˜ ì‚¬ì´íŠ¸ ì „ìš©
"""

import requests
import re
import time
from collections import Counter
from urllib.parse import quote
import csv

class SimpleKeywordExtractor:
    def __init__(self, website_url="https://www.dongraelaw.shop/"):
        self.website_url = website_url
        self.brand_name = "ë²•ë¬´ë²•ì¸ ë™ë˜"
        self.location = "ë¶€ì‚° ì—°ì œ"
        self.all_keywords = []
        
    def extract_from_website(self):
        """ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        print("ğŸ” 1ë‹¨ê³„: ì›¹ì‚¬ì´íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            
            response = requests.get(self.website_url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                # HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
                html = response.text
                
                # ì œëª© ì¶”ì¶œ
                title_match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
                title = title_match.group(1) if title_match else ""
                
                # ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
                meta_desc = re.search(r'<meta name="description" content="(.*?)"', html, re.IGNORECASE)
                description = meta_desc.group(1) if meta_desc else ""
                
                # HTML íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                text = re.sub(r'<[^>]+>', ' ', html)
                text = re.sub(r'\s+', ' ', text)
                
                # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ
                korean_words = re.findall(r'[ê°€-í£]{2,}', text)
                
                print(f"   ğŸ“„ ì œëª©: {title}")
                print(f"   ğŸ“ ì„¤ëª…: {description}")
                print(f"   ğŸ“Š ì¶”ì¶œëœ í•œê¸€ ë‹¨ì–´: {len(korean_words)}ê°œ")
                
                # ë¹ˆë„ìˆ˜ ê³„ì‚°
                word_count = Counter(korean_words)
                
                # ë¶ˆìš©ì–´ ì œê±°
                stopwords = {
                    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì´ê³³', 'ê·¸ê³³', 'ì €ê³³',
                    'ë•Œë¬¸', 'ê²½ìš°', 'ì‹œê°„', 'ì •ë„', 'ìƒíƒœ', 'ë°©ë²•', 'ì´í›„', 'ë‹¤ìŒ',
                    'ëª¨ë“ ', 'ê°ê°', 'ì „ì²´', 'ì¼ë¶€', 'í•˜ë‚˜', 'ë‹¤ë¥¸', 'ê°™ì€', 'ìƒˆë¡œìš´'
                }
                
                # ìƒìœ„ í‚¤ì›Œë“œ ì„ ë³„ (ë¶ˆìš©ì–´ ì œì™¸)
                website_keywords = []
                for word, count in word_count.most_common(20):
                    if word not in stopwords and len(word) >= 2:
                        website_keywords.append((word, count))
                
                print("   ğŸ” ì›¹ì‚¬ì´íŠ¸ ì£¼ìš” í‚¤ì›Œë“œ:")
                for i, (word, count) in enumerate(website_keywords[:10], 1):
                    print(f"      {i}. {word} ({count}íšŒ)")
                
                return [word for word, count in website_keywords]
                
            else:
                print(f"   âŒ ì›¹ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨: HTTP {response.status_code}")
                return []
                
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def extract_base_keywords(self):
        """ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„±"""
        print("\nğŸ”§ 2ë‹¨ê³„: ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„± ì¤‘...")
        
        base_keywords = [
            # ë¸Œëœë“œ ê´€ë ¨
            "ë²•ë¬´ë²•ì¸ ë™ë˜",
            "ë™ë˜ ë²•ë¬´ë²•ì¸",
            "ë²•ë¬´ë²•ì¸ë™ë˜",
            
            # ì§€ì—­ + ì„œë¹„ìŠ¤
            "ì—°ì œ ë³€í˜¸ì‚¬",
            "ì—°ì œêµ¬ ë³€í˜¸ì‚¬", 
            "ë¶€ì‚° ì—°ì œ ë³€í˜¸ì‚¬",
            "ê±°ì œì—­ ë³€í˜¸ì‚¬",
            "ì‹œì²­ì—­ ë³€í˜¸ì‚¬",
            "ì—°ì‚°ì—­ ë³€í˜¸ì‚¬",
            
            # ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ
            "ì—°ì œ ë²•ë¥ ìƒë‹´",
            "ì—°ì œ ë²•ë¬´ë²•ì¸",
            "ë¶€ì‚° ì—°ì œ ë²•ë¬´ë²•ì¸",
            "ì—°ì œêµ¬ ë²•ë¥ ìƒë‹´",
            
            # ì „ë¬¸ ë¶„ì•¼
            "ì—°ì œ ë¯¼ì‚¬ì†Œì†¡",
            "ì—°ì œ í˜•ì‚¬ë³€í˜¸",
            "ì—°ì œ ì´í˜¼ë³€í˜¸ì‚¬",
            "ì—°ì œ ìƒì†ë³€í˜¸ì‚¬",
            "ì—°ì œ êµí†µì‚¬ê³ ",
            "ì—°ì œ ê¸°ì—…ë²•ë¬´",
            "ì—°ì œ ë¶€ë™ì‚°",
            "ì—°ì œ ì±„ê¶ŒíšŒìˆ˜"
        ]
        
        print(f"   âœ… {len(base_keywords)}ê°œ ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„±")
        print("   ğŸ“‹ ê¸°ë³¸ í‚¤ì›Œë“œ ëª©ë¡:")
        for i, keyword in enumerate(base_keywords, 1):
            print(f"      {i}. {keyword}")
        
        return base_keywords
    
    def extract_naver_autocomplete(self, seed_keywords):
        """ë„¤ì´ë²„ ìë™ì™„ì„±ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        print("\nğŸ” 3ë‹¨ê³„: ë„¤ì´ë²„ ìë™ì™„ì„± í‚¤ì›Œë“œ ìˆ˜ì§‘ ì¤‘...")
        
        all_suggestions = []
        
        # ì£¼ìš” ì‹œë“œ í‚¤ì›Œë“œë§Œ ì‚¬ìš© (API í˜¸ì¶œ ìµœì†Œí™”)
        main_seeds = [
            "ì—°ì œ ë³€í˜¸ì‚¬",
            "ë²•ë¬´ë²•ì¸ ë™ë˜",
            "ì—°ì œ ë²•ë¥ ìƒë‹´",
            "ë¶€ì‚° ì—°ì œ ë³€í˜¸ì‚¬"
        ]
        
        for seed in main_seeds:
            print(f"   ğŸ” '{seed}' ìë™ì™„ì„± ê²€ìƒ‰...")
            
            try:
                url = "https://ac.search.naver.com/nx/ac"
                params = {
                    'q': seed,
                    'con': '0',
                    'frm': 'nv',
                    'ans': '2'
                }
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                }
                
                response = requests.get(url, params=params, headers=headers, timeout=5)
                
                if response.status_code == 200:
                    data = response.json()
                    suggestions = []
                    
                    for item in data.get('items', []):
                        for suggestion in item:
                            if isinstance(suggestion, list) and len(suggestion) > 0:
                                keyword = suggestion[0]
                                if keyword and isinstance(keyword, str):
                                    suggestions.append(keyword.strip())
                    
                    unique_suggestions = list(set(suggestions))
                    all_suggestions.extend(unique_suggestions)
                    
                    print(f"      â†’ {len(unique_suggestions)}ê°œ í‚¤ì›Œë“œ ë°œê²¬")
                    for suggestion in unique_suggestions[:5]:
                        print(f"         â€¢ {suggestion}")
                
                time.sleep(1)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
                
            except Exception as e:
                print(f"      âŒ {seed} ìë™ì™„ì„± ì‹¤íŒ¨: {e}")
        
        # ì¤‘ë³µ ì œê±°
        unique_all = list(set(all_suggestions))
        print(f"   âœ… ì´ {len(unique_all)}ê°œ ë„¤ì´ë²„ ìë™ì™„ì„± í‚¤ì›Œë“œ ìˆ˜ì§‘")
        
        return unique_all
    
    def generate_pattern_keywords(self, base_keywords):
        """íŒ¨í„´ ê¸°ë°˜ í‚¤ì›Œë“œ í™•ì¥"""
        print("\nğŸ”§ 4ë‹¨ê³„: íŒ¨í„´ ê¸°ë°˜ í‚¤ì›Œë“œ í™•ì¥ ì¤‘...")
        
        # ë²•ë¥  ê´€ë ¨ ìš©ì–´
        law_terms = [
            "ìƒë‹´", "ë¹„ìš©", "ìˆ˜ì„ë£Œ", "ì¶”ì²œ", "í›„ê¸°", "í‰ê°€", "ìˆœìœ„",
            "ë¯¼ì‚¬", "í˜•ì‚¬", "ì´í˜¼", "ìƒì†", "êµí†µì‚¬ê³ ", "ë¶€ë™ì‚°", 
            "ê¸°ì—…ë²•ë¬´", "ì±„ê¶ŒíšŒìˆ˜", "ì†í•´ë°°ìƒ", "ê³„ì•½", "ì†Œì†¡", "ë³€í˜¸"
        ]
        
        # ì§€ì—­ í™•ì¥
        locations = [
            "ë¶€ì‚°", "ì—°ì œêµ¬", "ì—°ì œ", "ê±°ì œì—­", "ì‹œì²­ì—­", "ì—°ì‚°ì—­",
            "í† í˜„ë™", "ê±°ì œë™", "ì—°ì‚°ë™", "ê²½ë‚¨"
        ]
        
        # ì˜ë„ í‚¤ì›Œë“œ
        intents = [
            "ë¬´ë£Œìƒë‹´", "ì´ˆê¸°ìƒë‹´", "ì „í™”ìƒë‹´", "ë°©ë¬¸ìƒë‹´",
            "24ì‹œê°„", "ì£¼ë§ìƒë‹´", "ì•¼ê°„ìƒë‹´", "ì˜¨ë¼ì¸ìƒë‹´",
            "ì „ë¬¸", "ê²½í—˜", "ì‹¤ë ¥", "ë¯¿ì„ë§Œí•œ", "ìœ ëª…í•œ"
        ]
        
        pattern_keywords = []
        
        # ì§€ì—­ + ë²•ë¥  ìš©ì–´ ì¡°í•©
        for location in locations[:3]:  # ì£¼ìš” ì§€ì—­ë§Œ
            for law_term in law_terms:
                pattern_keywords.extend([
                    f"{location} {law_term}",
                    f"{location} ë³€í˜¸ì‚¬ {law_term}",
                    f"{location} ë²•ë¬´ë²•ì¸ {law_term}"
                ])
        
        # ë¸Œëœë“œ + ì˜ë„ ì¡°í•©
        brand_terms = ["ë²•ë¬´ë²•ì¸ ë™ë˜", "ë™ë˜", "ì—°ì œ"]
        for brand in brand_terms:
            for intent in intents:
                pattern_keywords.extend([
                    f"{brand} {intent}",
                    f"{intent} {brand}"
                ])
        
        # ì¤‘ë³µ ì œê±°
        unique_pattern = list(set(pattern_keywords))
        
        print(f"   âœ… {len(unique_pattern)}ê°œ íŒ¨í„´ í‚¤ì›Œë“œ ìƒì„±")
        print("   ğŸ“‹ íŒ¨í„´ í‚¤ì›Œë“œ ì˜ˆì‹œ:")
        for i, keyword in enumerate(unique_pattern[:10], 1):
            print(f"      {i}. {keyword}")
        
        return unique_pattern
    
    def create_conversational_keywords(self, base_keywords):
        """ëŒ€í™”í˜• í‚¤ì›Œë“œ ìƒì„± (AI ê²€ìƒ‰ìš©)"""
        print("\nğŸ’¬ 5ë‹¨ê³„: ëŒ€í™”í˜• í‚¤ì›Œë“œ ìƒì„± ì¤‘...")
        
        conversation_patterns = [
            "{}ì— ëŒ€í•´ ì•Œë ¤ì¤˜",
            "{}ë¥¼ ì¶”ì²œí•´ì¤˜",
            "{}ê°€ ì–´ë•Œ?",
            "{}ì˜ ì¥ë‹¨ì ì´ ë­ì•¼?",
            "{}ë¥¼ ì„ íƒí•´ì•¼ í• ê¹Œ?",
            "{}ì—ì„œ ìƒë‹´ë°›ê³  ì‹¶ì–´",
            "{}ì˜ ë¹„ìš©ì€ ì–¼ë§ˆì•¼?",
            "{}ëŠ” ë¯¿ì„ë§Œí•´?",
            "{}ì˜ í‰íŒì€ ì–´ë•Œ?",
            "{}ì—ì„œ ì†Œì†¡í•˜ë©´ ì–´ë–¨ê¹Œ?"
        ]
        
        conversational_keywords = []
        
        # ì£¼ìš” í‚¤ì›Œë“œë§Œ ì‚¬ìš©
        main_keywords = [
            "ë²•ë¬´ë²•ì¸ ë™ë˜",
            "ë¶€ì‚° ë³€í˜¸ì‚¬", 
            "ë¶€ì‚° ë²•ë¬´ë²•ì¸",
            "ë¶€ì‚° ë™ë˜ ë³€í˜¸ì‚¬"
        ]
        
        for keyword in main_keywords:
            for pattern in conversation_patterns:
                conversational_keywords.append(pattern.format(keyword))
        
        print(f"   âœ… {len(conversational_keywords)}ê°œ ëŒ€í™”í˜• í‚¤ì›Œë“œ ìƒì„±")
        print("   ğŸ“‹ ëŒ€í™”í˜• í‚¤ì›Œë“œ ì˜ˆì‹œ:")
        for i, keyword in enumerate(conversational_keywords[:8], 1):
            print(f"      {i}. {keyword}")
        
        return conversational_keywords
    
    def save_keywords_to_file(self, all_keywords, filename="extracted_keywords.csv"):
        """í‚¤ì›Œë“œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        print(f"\nğŸ’¾ 6ë‹¨ê³„: í‚¤ì›Œë“œë¥¼ {filename}ì— ì €ì¥ ì¤‘...")
        
        try:
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(['ë²ˆí˜¸', 'í‚¤ì›Œë“œ', 'ê¸¸ì´', 'ë‹¨ì–´ìˆ˜', 'ìœ í˜•'])
                
                for i, (keyword, keyword_type) in enumerate(all_keywords, 1):
                    word_count = len(keyword.split())
                    writer.writerow([i, keyword, len(keyword), word_count, keyword_type])
            
            print(f"   âœ… {len(all_keywords)}ê°œ í‚¤ì›Œë“œê°€ {filename}ì— ì €ì¥ë¨")
            
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def run_extraction(self):
        """ì „ì²´ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰"""
        print("=" * 60)
        print("ğŸš€ ë²•ë¬´ë²•ì¸ ë™ë˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘!")
        print(f"ğŸ”— ëŒ€ìƒ ì›¹ì‚¬ì´íŠ¸: {self.website_url}")
        print("=" * 60)
        
        all_keywords = []
        
        # 1. ì›¹ì‚¬ì´íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        website_keywords = self.extract_from_website()
        for keyword in website_keywords:
            all_keywords.append((keyword, "ì›¹ì‚¬ì´íŠ¸"))
        
        # 2. ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„±
        base_keywords = self.extract_base_keywords()
        for keyword in base_keywords:
            all_keywords.append((keyword, "ê¸°ë³¸"))
        
        # 3. ë„¤ì´ë²„ ìë™ì™„ì„±
        naver_keywords = self.extract_naver_autocomplete(base_keywords)
        for keyword in naver_keywords:
            all_keywords.append((keyword, "ë„¤ì´ë²„"))
        
        # 4. íŒ¨í„´ í™•ì¥
        pattern_keywords = self.generate_pattern_keywords(base_keywords)
        for keyword in pattern_keywords:
            all_keywords.append((keyword, "íŒ¨í„´"))
        
        # 5. ëŒ€í™”í˜• í‚¤ì›Œë“œ
        conversational_keywords = self.create_conversational_keywords(base_keywords)
        for keyword in conversational_keywords:
            all_keywords.append((keyword, "ëŒ€í™”í˜•"))
        
        # ì¤‘ë³µ ì œê±° (í‚¤ì›Œë“œë§Œ ê¸°ì¤€ìœ¼ë¡œ)
        seen_keywords = set()
        unique_keywords = []
        for keyword, keyword_type in all_keywords:
            if keyword.lower() not in seen_keywords:
                seen_keywords.add(keyword.lower())
                unique_keywords.append((keyword, keyword_type))
        
        # 6. ê²°ê³¼ ì €ì¥
        self.save_keywords_to_file(unique_keywords)
        
        # 7. ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ!")
        print("=" * 60)
        
        # ìœ í˜•ë³„ í†µê³„
        type_counts = {}
        for keyword, keyword_type in unique_keywords:
            type_counts[keyword_type] = type_counts.get(keyword_type, 0) + 1
        
        print("ğŸ“Š ì¶”ì¶œ ê²°ê³¼ í†µê³„:")
        for keyword_type, count in type_counts.items():
            print(f"   â€¢ {keyword_type}: {count}ê°œ")
        
        print(f"\nğŸ”¢ ì´ í‚¤ì›Œë“œ ê°œìˆ˜: {len(unique_keywords)}ê°œ")
        
        # ì „ì²´ í‚¤ì›Œë“œ ì¶œë ¥
        print(f"\nğŸ“‹ ì „ì²´ ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡:")
        print("-" * 60)
        
        current_type = ""
        for i, (keyword, keyword_type) in enumerate(unique_keywords, 1):
            if keyword_type != current_type:
                current_type = keyword_type
                print(f"\n[{keyword_type} í‚¤ì›Œë“œ]")
            print(f"{i:3d}. {keyword}")
        
        print("\n" + "=" * 60)
        print("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ“ 'extracted_keywords.csv' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        print("=" * 60)
        
        return unique_keywords

# ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    extractor = SimpleKeywordExtractor()
    keywords = extractor.run_extraction()
    return keywords

if __name__ == "__main__":
    print("ğŸ”¥ ë²•ë¬´ë²•ì¸ ë™ë˜ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    keywords = main()